actor_hidden_size: [256, 256]  # The size of hidden layers for actor network.
activation: 'leaky_relu'      # The activation function for each hidden layer.
activation_action: 'sigmoid'  # The activation function for the last layer of actor network.
agent: "MASAC"  # the learning algorithms_marl
alpha: 0.01
batch_size: 1024
buffer_size: 1000000
continuous_action: True  # Continuous action space or not.
critic_hidden_size: [256, 256]
device: "cuda:0"
distributed_training: False  # Whether to use multi-GPU for distributed training.
dl_toolbox: "torch"  # The deep learning toolbox. Choices: "torch", "mindspore", "tensorflow"
env_name: "mpe"  # Name of the environment.
env_id: "simple_tag_v3"
env_seed: 42  # The random seed of the environment.
eval_interval: 100000
fps: 30  # The frames per second for the rendering videos in log file.
gamma: 0.95  # discount factor
grad_clip_norm: 0.5
learner: "MASAC_Learner"
learning_rate: 0.0004  # The learning rate.
learning_rate_actor: 0.01  # learning rate for actor
learning_rate_critic: 0.001  # learning rate for critic
log_dir: "./masac_logs/"
logger: "wandb"  # Choices: "tensorboard", "wandb".
master_port: '12355'  # The master port for current experiment when use distributed training.
model_dir: "./masac_models/"
video_dir: "./masac_videos/"
parallels: 16
policy: "Gaussian_MASAC_Policy"
project_name: "simple_tag_masac"
run_name: "test1"
render: True
render_mode: 'rgb_array' # Choices: 'human', 'rgb_array'.
representation: "Basic_Identical"
representation_hidden_size: []  # the units for each hidden layer
runner: "RunnerCompetition"
running_steps: 1600000
seed: 42
start_training: 10000  # start training after n steps
tau: 0.005  # soft update for target networks
test_episode: 20
test_mode: False  # Whether to run in test mode.
training_frequency: 100
use_actions_mask: False
use_grad_clip: True
use_parameter_sharing: False
use_automatic_entropy_tuning: True
vectorize: "DummyVecMultiAgentEnv"
wandb_user_name: "qinhn040516-nanjing-university"



